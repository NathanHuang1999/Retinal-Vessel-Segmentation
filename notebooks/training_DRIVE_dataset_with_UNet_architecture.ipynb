{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training_DRIVE_dataset_with_UNet_architecture.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiaQY50SBkqQ"
      },
      "source": [
        "# Mounting Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIloauIhIAYm",
        "outputId": "0c2938ec-cdc6-40fb-8d11-957340dac809"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KPuRKEfBoet"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EH1r-AuFIFyw"
      },
      "source": [
        "import os\n",
        "import skimage.io as io\n",
        "import skimage.transform as trans\n",
        "import shutil\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import time\n",
        "from __future__ import print_function\n",
        "import glob\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.optimizers import *\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras import backend as keras\n",
        "from tensorflow.keras.models import load_model as load_initial_model\n",
        "from google.colab.patches import cv2_imshow\n",
        "import gc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_HCART1BqqU"
      },
      "source": [
        "# Handcrafted Metrics For Additional Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcCvZQS9rY5V"
      },
      "source": [
        "def dice_coef(y_true, y_pred):\n",
        "  smooth = 0.0\n",
        "  y_true_f = keras.flatten(y_true)\n",
        "  y_pred_f = keras.flatten(y_pred)\n",
        "  intersection = keras.sum(y_true_f * y_pred_f)\n",
        "  return (2. * intersection + smooth) / (keras.sum(y_true_f) + keras.sum(y_pred_f) + smooth)\n",
        "\n",
        "def jacard(y_true, y_pred):\n",
        "\n",
        "  y_true_f = keras.flatten(y_true)\n",
        "  y_pred_f = keras.flatten(y_pred)\n",
        "  intersection = keras.sum ( y_true_f * y_pred_f)\n",
        "  union = keras.sum ( y_true_f + y_pred_f - y_true_f * y_pred_f)\n",
        "\n",
        "  return intersection/union"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVt1cPWwByok"
      },
      "source": [
        "# The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnPgkAyMIGqB"
      },
      "source": [
        "def unet(pretrained_weights = None,input_size = (608,576,1)):\n",
        "  inputs = Input(input_size)\n",
        "  conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "  conv1 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "  pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "  conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "  conv2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "  pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "  conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "  conv3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "  pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "  conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "  conv4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "  drop4 = Dropout(0.5)(conv4)\n",
        "  pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "  conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
        "  conv5 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "  drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "  up6 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
        "  merge6 = concatenate([drop4,up6], axis = 3)\n",
        "  conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
        "  conv6 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
        "\n",
        "  up7 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
        "  merge7 = concatenate([conv3,up7], axis = 3)\n",
        "  conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
        "  conv7 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
        "\n",
        "  up8 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
        "  merge8 = concatenate([conv2,up8], axis = 3)\n",
        "  conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "  conv8 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
        "\n",
        "  up9 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
        "  merge9 = concatenate([conv1,up9], axis = 3)\n",
        "  conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "  conv9 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "  conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "  conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
        "\n",
        "  model = Model(inputs,conv10)\n",
        "\n",
        "  model.compile(optimizer = Adam(lr = 1e-4), loss = 'binary_crossentropy', metrics = ['accuracy',dice_coef,jacard, tf.keras.metrics.AUC(), tf.keras.metrics.MeanIoU(num_classes=2),\n",
        "                                                                                      tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "    \n",
        "  #model.summary()\n",
        "\n",
        "  if(pretrained_weights):\n",
        "    model.load_weights(pretrained_weights)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNGQrmFeB0MJ"
      },
      "source": [
        "# Generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuZjmPUBIQsX"
      },
      "source": [
        "def adjustData(img,mask,flag_multi_class,num_class):\n",
        "  if(flag_multi_class):\n",
        "    img = img / 255\n",
        "    mask = mask[:,:,:,0] if(len(mask.shape) == 4) else mask[:,:,0]\n",
        "    new_mask = np.zeros(mask.shape + (num_class,))\n",
        "    for i in range(num_class):\n",
        "        #for one pixel in the image, find the class in mask and convert it into one-hot vector\n",
        "        #index = np.where(mask == i)\n",
        "        #index_mask = (index[0],index[1],index[2],np.zeros(len(index[0]),dtype = np.int64) + i) if (len(mask.shape) == 4) else (index[0],index[1],np.zeros(len(index[0]),dtype = np.int64) + i)\n",
        "        #new_mask[index_mask] = 1\n",
        "        new_mask[mask == i,i] = 1\n",
        "    new_mask = np.reshape(new_mask,(new_mask.shape[0],new_mask.shape[1]*new_mask.shape[2],new_mask.shape[3])) if flag_multi_class else np.reshape(new_mask,(new_mask.shape[0]*new_mask.shape[1],new_mask.shape[2]))\n",
        "    mask = new_mask\n",
        "  elif (np.max(img) > 1):\n",
        "    img = img / 255\n",
        "    mask = mask /255\n",
        "    mask[mask > 0.5] = 1\n",
        "    mask[mask <= 0.5] = 0\n",
        "  return (img,mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX__zdI9BXI8"
      },
      "source": [
        "def trainGenerator(batch_size,train_path,image_folder,mask_folder,aug_dict,image_color_mode = \"grayscale\",\n",
        "                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n",
        "                    flag_multi_class = False,num_class = 2,save_to_dir = None,target_size = (608,576),seed = 1):\n",
        "  image_datagen = ImageDataGenerator(**aug_dict)\n",
        "  mask_datagen = ImageDataGenerator(**aug_dict)\n",
        "  image_generator = image_datagen.flow_from_directory(\n",
        "      train_path,\n",
        "      classes = [image_folder],\n",
        "      class_mode = None,\n",
        "      color_mode = image_color_mode,\n",
        "      target_size = target_size,\n",
        "      batch_size = batch_size,\n",
        "      save_to_dir = save_to_dir,\n",
        "      save_prefix  = image_save_prefix,\n",
        "      seed = seed)\n",
        "  mask_generator = mask_datagen.flow_from_directory(\n",
        "      train_path,\n",
        "      classes = [mask_folder],\n",
        "      class_mode = None,\n",
        "      color_mode = mask_color_mode,\n",
        "      target_size = target_size,\n",
        "      batch_size = batch_size,\n",
        "      save_to_dir = save_to_dir,\n",
        "      save_prefix  = mask_save_prefix,\n",
        "      seed = seed)\n",
        "  train_generator = zip(image_generator, mask_generator)\n",
        "  for (img,mask) in train_generator:\n",
        "    img,mask = adjustData(img,mask,flag_multi_class,num_class)\n",
        "    yield (img,mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ns8cuqZDBVCM"
      },
      "source": [
        "def testGenerator(test_path, target_size = (608,576),flag_multi_class = False,as_gray = True):\n",
        "  image_datagen = ImageDataGenerator(rescale=1./255)\n",
        "  mask_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "  for img_name in sorted(os.listdir(test_path + \"/images\")):\n",
        "      img = io.imread(os.path.join(test_path + \"/images\",img_name),as_gray = as_gray)\n",
        "      img = img / 255\n",
        "      img = trans.resize(img,target_size)\n",
        "      img = np.reshape(img,img.shape+(1,)) if (not flag_multi_class) else img\n",
        "      img = np.reshape(img,(1,)+img.shape)\n",
        "      yield img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1ujxwRxBS3Z"
      },
      "source": [
        "def testGenerator2(batch_size,test_path,image_folder,mask_folder,aug_dict,image_color_mode = \"grayscale\",\n",
        "                    mask_color_mode = \"grayscale\",image_save_prefix  = \"image\",mask_save_prefix  = \"mask\",\n",
        "                    flag_multi_class = False,num_class = 2,save_to_dir = None,target_size = (608,576),seed = 1):\n",
        "  image_datagen = ImageDataGenerator(**aug_dict)\n",
        "  mask_datagen = ImageDataGenerator(**aug_dict)\n",
        "  image_generator = image_datagen.flow_from_directory(\n",
        "      test_path,\n",
        "      classes = [image_folder],\n",
        "      class_mode = None,\n",
        "      color_mode = image_color_mode,\n",
        "      target_size = target_size,\n",
        "      batch_size = batch_size,\n",
        "      save_to_dir = save_to_dir,\n",
        "      save_prefix  = image_save_prefix,\n",
        "      seed = seed)\n",
        "  mask_generator = mask_datagen.flow_from_directory(\n",
        "      test_path,\n",
        "      classes = [mask_folder],\n",
        "      class_mode = None,\n",
        "      color_mode = mask_color_mode,\n",
        "      target_size = target_size,\n",
        "      batch_size = batch_size,\n",
        "      save_to_dir = save_to_dir,\n",
        "      save_prefix  = mask_save_prefix,\n",
        "      seed = seed)\n",
        "  test_generator = zip(image_generator, mask_generator)\n",
        "  for (img,mask) in test_generator:\n",
        "    img,mask = adjustData(img,mask,flag_multi_class,num_class)\n",
        "    yield (img,mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNv00UXuBJCI"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWdo9rMNC8UH"
      },
      "source": [
        "def pad(input_folder, output_folder):\n",
        "  for file in sorted(os.listdir(input_folder)):\n",
        "    if '.png' in file:\n",
        "      tmp = cv2.imread(input_folder + '/' + file, 0)\n",
        "      tmp = cv2.copyMakeBorder(tmp.copy(),12,12,5,6,cv2.BORDER_CONSTANT,value=(0,0,0))\n",
        "      io.imsave(output_folder + '/' + file, tmp)\n",
        "  print('Padding is done.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_fxvrB2A5bL"
      },
      "source": [
        "def crop(input_folder, output_folder):\n",
        "  for file in sorted(os.listdir(input_folder)):\n",
        "    if '.png' in file:\n",
        "      tmp = cv2.imread(input_folder + '/' + file, 0)\n",
        "      tmp = tmp[12:-12, 5:-6]\n",
        "      io.imsave(output_folder + '/' + file, tmp)\n",
        "  print('Cropping is done.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syDqXJGVLW9U"
      },
      "source": [
        "def saveResult_drive(save_path,npyfile,flag_multi_class = False,num_class = 2):\n",
        "  for i,item in enumerate(npyfile):\n",
        "      img = labelVisualize(num_class,COLOR_DICT,item) if flag_multi_class else item[:,:,0]\n",
        "      io.imsave(os.path.join(save_path,\"%d.png\"%(i)),img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3DKunl8dVFU"
      },
      "source": [
        "def threshold(folder):\n",
        "  for img in sorted(os.listdir(folder)):\n",
        "    tmp = cv2.imread(folder + '/' + img, 0)\n",
        "    _, tmp = cv2.threshold(tmp,115,255,cv2.THRESH_BINARY)\n",
        "    io.imsave(folder + '/' + img, tmp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZdSCsX8eQG7"
      },
      "source": [
        "def set_order(input_folder, output_folder):\n",
        "\n",
        "  for img in sorted(os.listdir(input_folder)):\n",
        "\n",
        "    if int(img.split('.')[0]) == 0:\n",
        "      tmp = cv2.imread(input_folder  + '/' + img, 0)\n",
        "      io.imsave(output_folder + '/' + str(int(img.split('.')[0])+1) + '.png', tmp)\n",
        "    elif int(img.split('.')[0]) >= 1 and int(img.split('.')[0]) <= 10:\n",
        "      tmp = cv2.imread(input_folder  + '/' + img, 0)\n",
        "      io.imsave(output_folder + '/' + str(int(img.split('.')[0])+9) + '.png', tmp)\n",
        "    elif int(img.split('.')[0]) >= 13 and int(img.split('.')[0]) <= 19:\n",
        "      tmp = cv2.imread(input_folder  + '/' + img, 0)\n",
        "      io.imsave(output_folder + '/' + str(int(img.split('.')[0])-10) + '.png', tmp)\n",
        "    elif int(img.split('.')[0]) == 11:\n",
        "      tmp = cv2.imread(input_folder  + '/' + img, 0)\n",
        "      io.imsave(output_folder + '/' + str(int(img.split('.')[0])-9) + '.png', tmp)\n",
        "    elif int(img.split('.')[0]) == 12:\n",
        "      tmp = cv2.imread(input_folder  + '/' + img, 0)\n",
        "      io.imsave(output_folder + '/' + str(int(img.split('.')[0])+8) + '.png', tmp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBpbFMqXLIuk"
      },
      "source": [
        "def dice(true_mask, pred_mask, non_seg_score=1.0):\n",
        "    \"\"\"\n",
        "        Computes the Dice coefficient.\n",
        "        Args:\n",
        "            true_mask : Array of arbitrary shape.\n",
        "            pred_mask : Array with the same shape than true_mask.  \n",
        "        \n",
        "        Returns:\n",
        "            A scalar representing the Dice coefficient between the two segmentations. \n",
        "        \n",
        "    \"\"\"\n",
        "    assert true_mask.shape == pred_mask.shape\n",
        "\n",
        "    true_mask = np.asarray(true_mask).astype(np.bool)\n",
        "    pred_mask = np.asarray(pred_mask).astype(np.bool)\n",
        "\n",
        "    # If both segmentations are all zero, the dice will be 1. (Developer decision)\n",
        "    im_sum = true_mask.sum() + pred_mask.sum()\n",
        "    if im_sum == 0:\n",
        "        return non_seg_score\n",
        "\n",
        "    # Compute Dice coefficient\n",
        "    intersection = np.logical_and(true_mask, pred_mask)\n",
        "    return 2. * intersection.sum() / im_sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qkknx_VeZoJ"
      },
      "source": [
        "def mean_dice(true_path, pred_path):\n",
        "  \n",
        "  sum = 0\n",
        "  \n",
        "  for img in sorted(os.listdir(pred_path)):\n",
        "    \n",
        "    true_tmp = cv2.imread(true_path + '/' + img, 0)\n",
        "    pred_tmp = cv2.imread(pred_path + '/' + img, 0)\n",
        "    \n",
        "    a = dice(true_tmp, pred_tmp)\n",
        "    print(a)\n",
        "    sum += a\n",
        "  \n",
        "  return sum/len(os.listdir(true_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiIBbH4DKkUD"
      },
      "source": [
        "# DRIVE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pY48ylfTKmKw"
      },
      "source": [
        "LOG_PATH      = '/content/drive/MyDrive/AI_Projects/retina_DRIVE/U-Net/logs'\n",
        "RESULT_PATH   = '/content/drive/MyDrive/AI_Projects/retina_DRIVE/U-Net/test_results'\n",
        "MODEL_PATH    = '/content/drive/MyDrive/AI_Projects/retina_DRIVE/U-Net/models'\n",
        "\n",
        "TRAIN_PATH    = '/content/drive/MyDrive/AI_Projects/retina_DRIVE/png_data/combinations/ilk_yuz'\n",
        "VAL_PATH      = '/content/drive/MyDrive/AI_Projects/retina_DRIVE/png_data/test'\n",
        "TEST_PATH     = '/content/drive/MyDrive/AI_Projects/retina_DRIVE/png_data/test'\n",
        "\n",
        "TMP_TRAIN     = '/content/drive/MyDrive/AI_Projects/retina_DRIVE/U-Net/tmp_train'\n",
        "TMP_TEST      = '/content/drive/MyDrive/AI_Projects/retina_DRIVE/U-Net/tmp_test'\n",
        "TMP_VAL       = '/content/drive/MyDrive/AI_Projects/retina_DRIVE/U-Net/tmp_val'\n",
        "TMP_RESULT    = '/content/drive/MyDrive/AI_Projects/retina_DRIVE/U-Net/tmp_result'\n",
        "\n",
        "if (os.path.isdir(LOG_PATH) and os.path.isdir(RESULT_PATH) and \\\n",
        "    os.path.isdir(MODEL_PATH) and os.path.isdir(TRAIN_PATH)) and \\\n",
        "    os.path.isdir(TEST_PATH) and os.path.isdir(VAL_PATH) and \\\n",
        "    os.path.isdir(TMP_TRAIN) and os.path.isdir(TMP_TEST) and \\\n",
        "    os.path.isdir(TMP_VAL) and os.path.isdir(TMP_RESULT) == 0:\n",
        "    raise OSError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFj8N0Dbhitj"
      },
      "source": [
        "## Train Many Epochs at Once "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daLKmP7nLW9V"
      },
      "source": [
        "def train_eval_drive(save_name, num_train, num_test, initial_model_path, train_batch = 3, test_batch = 3, epoch = 5):\n",
        "  \n",
        "  shutil.rmtree(TMP_TRAIN, ignore_errors=False, onerror=None)\n",
        "  os.mkdir(TMP_TRAIN)\n",
        "  os.mkdir(TMP_TRAIN + \"/images\")\n",
        "  os.mkdir(TMP_TRAIN + \"/labels\")\n",
        "\n",
        "  shutil.rmtree(TMP_TEST, ignore_errors=False, onerror=None)\n",
        "  os.mkdir(TMP_TEST)\n",
        "  os.mkdir(TMP_TEST + \"/images\")\n",
        "  os.mkdir(TMP_TEST + \"/labels\")\n",
        "  \n",
        "  \n",
        "  shutil.rmtree(TMP_VAL, ignore_errors=False, onerror=None)\n",
        "  os.mkdir(TMP_VAL)\n",
        "  os.mkdir(TMP_VAL + \"/images\")\n",
        "  os.mkdir(TMP_VAL + \"/labels\")\n",
        "  \n",
        "\n",
        "  pad(TRAIN_PATH + '/images', TMP_TRAIN + '/images')\n",
        "  pad(TRAIN_PATH + '/labels', TMP_TRAIN + '/labels')\n",
        "\n",
        "  pad(TEST_PATH + '/images', TMP_TEST + '/images')\n",
        "  pad(TEST_PATH + '/labels', TMP_TEST + '/labels')\n",
        "  \n",
        "  pad(VAL_PATH + '/images', TMP_VAL + '/images')\n",
        "  pad(VAL_PATH + '/labels', TMP_VAL + '/labels')\n",
        "  \n",
        "  data_gen_args = dict()\n",
        "  train_generator = trainGenerator(train_batch, TMP_TRAIN, 'images', 'labels', data_gen_args, save_to_dir = None, target_size=(608,576))\n",
        "  test_generator = testGenerator2(test_batch, TMP_TEST, 'images', 'labels', data_gen_args, save_to_dir = None, target_size=(608,576))\n",
        "\n",
        "  model = unet(input_size=(608,576,1))\n",
        "  if initial_model_path != None:\n",
        "    model.load_weights(initial_model_path)\n",
        "\n",
        "  model_checkpoint = ModelCheckpoint(MODEL_PATH + \"/\" + save_name +\".hdf5\", monitor='loss',verbose=1, save_best_only=True)\n",
        "\n",
        "  model_history = model.fit_generator(train_generator, steps_per_epoch=num_train//train_batch, epochs=epoch, callbacks=[model_checkpoint],\n",
        "                                      validation_data=test_generator, validation_steps=num_test//test_batch)\n",
        "  \n",
        "  log_file = open(LOG_PATH + \"/log_{}.pkl\".format(save_name), \"wb\")#history file\n",
        "  pickle.dump(model_history.history, log_file)\n",
        "  log_file.close()\n",
        "\n",
        "  test_generator_2 = testGenerator(TMP_TEST, target_size=(608,576))\n",
        "  results = model.predict_generator(test_generator_2,verbose=1)\n",
        "\n",
        "  shutil.rmtree(TMP_RESULT, ignore_errors=False, onerror=None)\n",
        "  os.mkdir(TMP_RESULT)\n",
        "  saveResult_drive(TMP_RESULT, results)\n",
        "  \n",
        "  os.mkdir(RESULT_PATH + '/' + save_name)\n",
        "  crop(TMP_RESULT, RESULT_PATH + '/' + save_name)\n",
        "\n",
        "  threshold(RESULT_PATH + '/' + save_name)\n",
        "\n",
        "  shutil.rmtree(RESULT_PATH + '/download', ignore_errors=False, onerror=None)\n",
        "  os.mkdir(RESULT_PATH + '/download')\n",
        "  set_order(RESULT_PATH + '/' + save_name, RESULT_PATH + '/download')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_PwUgZKsA-g"
      },
      "source": [
        "train_sample_number = len(os.listdir(TRAIN_PATH + '/images'))\n",
        "test_sample_number  = len(os.listdir(TEST_PATH + '/images'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j14e8MXUDaOd"
      },
      "source": [
        "SAVE_NAME = 'my_model'\n",
        "INITIAL_MODEL_PATH = None\n",
        "EPOCH = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7V2TCeCLf0q"
      },
      "source": [
        "train_eval_drive(SAVE_NAME, initial_model_path= INITIAL_MODEL_PATH, epoch= EPOCH, train_batch = 3, test_batch = 3, num_train = train_sample_number, num_test= test_sample_number)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aDRbHdULf0r"
      },
      "source": [
        "log_file = open(LOG_PATH + \"/log_\" + SAVE_NAME + \".pkl\" , \"rb\")\n",
        "output = pickle.load(log_file)\n",
        "i = 0\n",
        "for key, value in output.items():\n",
        "  print(key + \" --> \" + str(value[EPOCH-1]))\n",
        "  i = i+1\n",
        "print(50*\"-\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sX4aZoLJeGZ1"
      },
      "source": [
        "mean_dice_coef = mean_dice(TEST_PATH +   '/labels', \n",
        "                           RESULT_PATH + '/download')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U6Z9QBnePTW"
      },
      "source": [
        "mean_dice_coef"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fif9u0rLgn4j"
      },
      "source": [
        "## Train with Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq9yvL9RgpNk"
      },
      "source": [
        "def train_eval_drive(save_name, initial_model_name, n_samples, train_batch = 3, test_batch = 3, epochs = 5):\n",
        "  \n",
        "  shutil.rmtree(TMP_TRAIN, ignore_errors=False, onerror=None)\n",
        "  os.mkdir(TMP_TRAIN)\n",
        "  os.mkdir(TMP_TRAIN + \"/images\")\n",
        "  os.mkdir(TMP_TRAIN + \"/labels\")\n",
        "\n",
        "  shutil.rmtree(TMP_TEST, ignore_errors=False, onerror=None)\n",
        "  os.mkdir(TMP_TEST)\n",
        "  os.mkdir(TMP_TEST + \"/images\")\n",
        "  os.mkdir(TMP_TEST + \"/labels\")\n",
        "  \n",
        "  pad(TRAIN_PATH + '/images', TMP_TRAIN + '/images')\n",
        "  pad(TRAIN_PATH + '/labels', TMP_TRAIN + '/labels')\n",
        "\n",
        "  pad(TEST_PATH + '/images', TMP_TEST + '/images')\n",
        "  pad(TEST_PATH + '/labels', TMP_TEST + '/labels')\n",
        "  \n",
        "  data_gen_args = dict()\n",
        "  train_generator = trainGenerator(train_batch, TMP_TRAIN, 'images', 'labels', data_gen_args, save_to_dir = None, target_size=(608,576))\n",
        "  test_generator = testGenerator2(test_batch, TMP_TEST, 'images', 'labels', data_gen_args, save_to_dir = None, target_size=(608,576))\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "  \n",
        "    model = unet(input_size=(608,576,1))\n",
        "    if epoch != 0:\n",
        "      model.load_weights(f'{MODEL_PATH}/{save_name +\"_\"+ str(epoch-1)}.hdf5')\n",
        "    else:\n",
        "      if initial_model_name != None:\n",
        "        model.load_weights(f'{MODEL_PATH}/{initial_model_name}.hdf5')\n",
        "\n",
        "    model_checkpoint = ModelCheckpoint(MODEL_PATH + \"/\" + save_name + \"_\" + str(epoch) +\".hdf5\", monitor='loss',verbose=1, save_best_only=True)\n",
        "\n",
        "    model_history = model.fit_generator(train_generator, steps_per_epoch=n_samples//train_batch, epochs=1, callbacks=[model_checkpoint],\n",
        "                                      validation_data=test_generator, validation_steps=20//test_batch)\n",
        "  \n",
        "    log_file = open(LOG_PATH + \"/log_{}.pkl\".format(save_name + str(epoch)), \"wb\")#history file\n",
        "    pickle.dump(model_history.history, log_file)\n",
        "    log_file.close()\n",
        "\n",
        "    test_generator_2 = testGenerator(TMP_TEST, target_size=(608,576))\n",
        "    results = model.predict_generator(test_generator_2,verbose=1)\n",
        "\n",
        "    shutil.rmtree(TMP_RESULT, ignore_errors=False, onerror=None)\n",
        "    os.mkdir(TMP_RESULT)\n",
        "    saveResult_drive(TMP_RESULT, results)\n",
        "    \n",
        "    os.mkdir(RESULT_PATH + '/' + save_name + str(epoch))\n",
        "    crop(TMP_RESULT, RESULT_PATH + '/' + save_name + str(epoch))\n",
        "\n",
        "    threshold(RESULT_PATH + '/' + save_name + str(epoch))\n",
        "\n",
        "    os.mkdir(RESULT_PATH + '/download' + \"_\"+save_name + str(epoch))\n",
        "    set_order(RESULT_PATH + '/' + save_name + str(epoch), RESULT_PATH + '/download' + \"_\"+save_name + str(epoch))\n",
        "\n",
        "    mean_dice_coef = mean_dice(TEST_PATH   + '/labels', \n",
        "                               RESULT_PATH + '/download_' + save_name + str(epoch))\n",
        "    print(f\"\\nMean dice coeff at epoch {epoch}: {mean_dice_coef}\\n\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_f5Sa6ii1FO"
      },
      "source": [
        "train_sample_number = len(os.listdir(TRAIN_PATH + '/images'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS_ZE_bssBkB"
      },
      "source": [
        "SAVE_NAME = 'my_model'\n",
        "initial_model_name = None\n",
        "\n",
        "train_eval_drive(save_name=SAVE_NAME, initial_model_name = initial_model_name, n_samples= train_sample_number, epochs=30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41uZa_HUESxu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}